{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "\n",
    "data_path = '/Users/ruoyangzhang/Documents/PythonWorkingDirectory/news_exploration/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we still need to systematically extract the specific article links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.reuters.com/resources/archive/us/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract links for specific years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [link['href'] for link in soup.find_all(href = True) if bool(re.search('/resources/archive/us/20', link['href'])) and len(link['href'].split('.')[-2].split('/')[-1]) == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing the links\n",
    "links = ['https://www.reuters.com' + link for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links[7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to extract the aggregated article links in the specific pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.76it/s]\n"
     ]
    }
   ],
   "source": [
    "aggregated_article_links = []\n",
    "for link in tqdm(links):\n",
    "    page_date = requests.get(link)\n",
    "    soup_date = BeautifulSoup(page_date.content, 'html.parser')\n",
    "    links_date = [link['href'] for link in soup_date.find_all(href = True) if bool(re.search('/resources/archive/us/20', link['href'])) and len(link['href'].split('.')[-2].split('/')[-1]) > 4]\n",
    "    aggregated_article_links.extend(links_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completing the links\n",
    "aggregated_article_links = ['https://www.reuters.com' + link for link in aggregated_article_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the article links, we will ignore the non text articles, such as videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_link_extraction(link):\n",
    "    page_article = requests.get(link)\n",
    "    soup_article =  BeautifulSoup(page_article.content, 'html.parser')\n",
    "    article_links = [link['href'] for link in soup_article.find_all(href = True) if 'article' in link['href'].split('/')]\n",
    "    return(article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1096/1096 [00:00<00:00, 126456.79it/s]\n"
     ]
    }
   ],
   "source": [
    "p = Pool(8, maxtasksperchild=1)\n",
    "extraction_res = p.map(article_link_extraction, tqdm(aggregated_article_links))\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = [item for sublist in extraction_res for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = list(set(article_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the links for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path+'article_links.pickle', 'wb') as handle:\n",
    "    pickle.dump(article_links, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1692132"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected 8 million article links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
